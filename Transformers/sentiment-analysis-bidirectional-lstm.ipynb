{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1472453,"sourceType":"datasetVersion","datasetId":863934}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport re\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom collections import Counter\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils.rnn import pad_sequence\nfrom datasets import Dataset as HFDataset\nimport pandas as pd\nfrom datasets import Dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T03:17:07.137711Z","iopub.execute_input":"2025-10-09T03:17:07.138440Z","iopub.status.idle":"2025-10-09T03:17:07.142822Z","shell.execute_reply.started":"2025-10-09T03:17:07.138415Z","shell.execute_reply":"2025-10-09T03:17:07.142239Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"datatattle/covid-19-nlp-text-classification\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T02:33:28.742759Z","iopub.execute_input":"2025-10-09T02:33:28.743318Z","iopub.status.idle":"2025-10-09T02:33:28.957833Z","shell.execute_reply.started":"2025-10-09T02:33:28.743297Z","shell.execute_reply":"2025-10-09T02:33:28.957262Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file = \"/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_test.csv\"\ndf = pd.read_csv(file)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T02:35:13.583373Z","iopub.execute_input":"2025-10-09T02:35:13.583868Z","iopub.status.idle":"2025-10-09T02:35:13.653312Z","shell.execute_reply.started":"2025-10-09T02:35:13.583830Z","shell.execute_reply":"2025-10-09T02:35:13.652544Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"columns:\", df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T02:35:17.083673Z","iopub.execute_input":"2025-10-09T02:35:17.084409Z","iopub.status.idle":"2025-10-09T02:35:17.088450Z","shell.execute_reply.started":"2025-10-09T02:35:17.084383Z","shell.execute_reply":"2025-10-09T02:35:17.087778Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[['OriginalTweet', 'Sentiment']].dropna().reset_index(drop=True)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T02:35:17.391272Z","iopub.execute_input":"2025-10-09T02:35:17.391764Z","iopub.status.idle":"2025-10-09T02:35:17.408333Z","shell.execute_reply.started":"2025-10-09T02:35:17.391743Z","shell.execute_reply":"2025-10-09T02:35:17.407538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_text(text):\n  text = re.sub(r'http\\S+', '', text)\n  text = re.sub(r'@\\w+', '', text)\n  text = re.sub(r'#(w+)', r'\\1', text)\n  text = re.sub(r\"[^A-Za-z0-9']+\", \" \", text)\n  text = re.sub(r\"\\s+\", \" \", text).strip().lower()\n  return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T02:35:19.687252Z","iopub.execute_input":"2025-10-09T02:35:19.687839Z","iopub.status.idle":"2025-10-09T02:35:19.692305Z","shell.execute_reply.started":"2025-10-09T02:35:19.687812Z","shell.execute_reply":"2025-10-09T02:35:19.691389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['clean_text'] = df['OriginalTweet'].apply(clean_text)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T02:35:20.012208Z","iopub.execute_input":"2025-10-09T02:35:20.012476Z","iopub.status.idle":"2025-10-09T02:35:20.124304Z","shell.execute_reply.started":"2025-10-09T02:35:20.012459Z","shell.execute_reply":"2025-10-09T02:35:20.123708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = sorted(df['Sentiment'].unique())\nlabel2id = {label: i for i, label in enumerate(labels)}\ndf['label_id'] = df['Sentiment'].map(label2id)\nprint(\"Label Mapping:\", label2id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T02:35:22.221813Z","iopub.execute_input":"2025-10-09T02:35:22.222143Z","iopub.status.idle":"2025-10-09T02:35:22.230845Z","shell.execute_reply.started":"2025-10-09T02:35:22.222122Z","shell.execute_reply":"2025-10-09T02:35:22.230100Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T02:35:23.825570Z","iopub.execute_input":"2025-10-09T02:35:23.826120Z","iopub.status.idle":"2025-10-09T02:35:23.834133Z","shell.execute_reply.started":"2025-10-09T02:35:23.826094Z","shell.execute_reply":"2025-10-09T02:35:23.833465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize(text):\n  return re.findall(r\"\\w+|[^\\w\\s]\", text)\n\ncounter = Counter()\nfor t in train_df[\"clean_text\"]:\n  counter.update(tokenize(t))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T02:35:31.077744Z","iopub.execute_input":"2025-10-09T02:35:31.078519Z","iopub.status.idle":"2025-10-09T02:35:31.165662Z","shell.execute_reply.started":"2025-10-09T02:35:31.078493Z","shell.execute_reply":"2025-10-09T02:35:31.164937Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"itos = ['<pad>', '<unk>'] + [w for w, c in counter.items() if c>= 2]\nstoi = {w: i for i, w in enumerate(itos)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T02:35:38.912727Z","iopub.execute_input":"2025-10-09T02:35:38.913446Z","iopub.status.idle":"2025-10-09T02:35:38.918334Z","shell.execute_reply.started":"2025-10-09T02:35:38.913422Z","shell.execute_reply":"2025-10-09T02:35:38.917474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encode(text):\n  return [stoi.get(t, stoi[\"<unk>\"]) for t in tokenize(text)]\n\nclass TweetDataset(Dataset):\n  def __init__(self, df):\n    self.text = df['clean_text'].tolist()\n    self.labels = df['label_id'].tolist()\n  def __len__(self):\n    return len(self.text)\n  def __getitem__(self, index):\n    return torch.tensor(encode(self.text[index])), torch.tensor(self.labels[index])\n\ndef collate(batch):\n  texts, labels = zip(*batch)\n  texts = pad_sequence(texts, batch_first=True, padding_value=stoi[\"<pad>\"])\n  labels = torch.stack(labels)\n  return texts, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T02:35:47.000335Z","iopub.execute_input":"2025-10-09T02:35:47.000913Z","iopub.status.idle":"2025-10-09T02:35:47.006283Z","shell.execute_reply.started":"2025-10-09T02:35:47.000884Z","shell.execute_reply":"2025-10-09T02:35:47.005456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_rnn = DataLoader(TweetDataset(train_df), batch_size=32, shuffle=True, collate_fn=collate)\nval_rnn = DataLoader(TweetDataset(val_df), batch_size=64, collate_fn=collate)\ntest_rnn = DataLoader(TweetDataset(test_df), batch_size=64, collate_fn=collate)\nprint(f\"Vocab size: {len(itos)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T02:35:54.260480Z","iopub.execute_input":"2025-10-09T02:35:54.260753Z","iopub.status.idle":"2025-10-09T02:35:54.266537Z","shell.execute_reply.started":"2025-10-09T02:35:54.260735Z","shell.execute_reply":"2025-10-09T02:35:54.265983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T02:36:02.290378Z","iopub.execute_input":"2025-10-09T02:36:02.291136Z","iopub.status.idle":"2025-10-09T02:36:02.295205Z","shell.execute_reply.started":"2025-10-09T02:36:02.291109Z","shell.execute_reply":"2025-10-09T02:36:02.294367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LSTM(nn.Module):\n  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, bidirectional,n_layers, dropout, pad_idx):\n    super().__init__()\n    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n    self.rnn = nn.LSTM(embedding_dim, hidden_dim, bidirectional = True,num_layers=n_layers, dropout=dropout, batch_first=True)\n    self.fc = nn.Linear(hidden_dim*2, output_dim)\n    self.dropout = nn.Dropout(dropout)\n\n  def forward(self, text):\n    embedded = self.dropout(self.embedding(text))\n    output, (hidden, cell) = self.rnn(embedded)\n    pooled = torch.max(output, dim=1)[0]  \n    pooled = self.dropout(pooled)\n    return self.fc(pooled)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T03:03:26.272385Z","iopub.execute_input":"2025-10-09T03:03:26.273118Z","iopub.status.idle":"2025-10-09T03:03:26.278178Z","shell.execute_reply.started":"2025-10-09T03:03:26.273090Z","shell.execute_reply":"2025-10-09T03:03:26.277548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classes = np.unique(df['label_id'])\nweights = compute_class_weight(\"balanced\", classes=classes, y=df['label_id'])\nweights = torch.tensor(weights, dtype=torch.float).to(device)\nn_layers = 3\nmodel = LSTM(len(itos), 16, 128, len(set(df['label_id'])), True,n_layers, 0.2, stoi[\"<pad>\"]).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\ncriterion = nn.CrossEntropyLoss(weight = weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T03:03:26.844529Z","iopub.execute_input":"2025-10-09T03:03:26.844782Z","iopub.status.idle":"2025-10-09T03:03:26.865461Z","shell.execute_reply.started":"2025-10-09T03:03:26.844763Z","shell.execute_reply":"2025-10-09T03:03:26.864906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def training(loader):\n  model.train()\n  total_loss, correct, total = 0, 0, 0\n  for texts, labels in train_rnn:\n    texts, labels = texts.to(device), labels.to(device)\n    \n    texts = texts.long()\n    \n    optimizer.zero_grad()\n    logits = model(texts)\n    loss = criterion(logits, labels)\n    loss.backward()\n    optimizer.step()\n\n    total_loss += loss.item() * texts.size(0)\n    preds = logits.argmax(1)\n    correct += (preds == labels).sum().item()\n    total += labels.size(0)\n  return total_loss / total, correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T03:03:28.524933Z","iopub.execute_input":"2025-10-09T03:03:28.525233Z","iopub.status.idle":"2025-10-09T03:03:28.530650Z","shell.execute_reply.started":"2025-10-09T03:03:28.525210Z","shell.execute_reply":"2025-10-09T03:03:28.529900Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate(loader):\n    model.eval()\n    total_loss, correct, total = 0, 0, 0\n    with torch.no_grad():\n        for texts, labels in loader:\n            texts, labels = texts.to(device), labels.to(device)\n            logits = model(texts)\n            loss = criterion(logits, labels)\n            total_loss += loss.item() * texts.size(0)\n            preds = logits.argmax(1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n    return total_loss / total, correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T03:03:29.224978Z","iopub.execute_input":"2025-10-09T03:03:29.225248Z","iopub.status.idle":"2025-10-09T03:03:29.230539Z","shell.execute_reply.started":"2025-10-09T03:03:29.225229Z","shell.execute_reply":"2025-10-09T03:03:29.229847Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(75):\n    train_loss, train_acc = training(train_rnn)\n    val_loss, val_acc = evaluate(val_rnn)\n    print(f\"Epoch {epoch+1}: Train Loss {train_loss:.4f}, Acc {train_acc:.3f} | Val Loss {val_loss:.4f}, Acc {val_acc:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T03:03:30.985149Z","iopub.execute_input":"2025-10-09T03:03:30.985818Z","iopub.status.idle":"2025-10-09T03:04:26.243006Z","shell.execute_reply.started":"2025-10-09T03:03:30.985792Z","shell.execute_reply":"2025-10-09T03:04:26.242275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_loss, test_acc = evaluate(test_rnn)\nprint(f\"Test Accuracy: {test_acc:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T03:04:29.919777Z","iopub.execute_input":"2025-10-09T03:04:29.920148Z","iopub.status.idle":"2025-10-09T03:04:30.001909Z","shell.execute_reply.started":"2025-10-09T03:04:29.920120Z","shell.execute_reply":"2025-10-09T03:04:30.001183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}