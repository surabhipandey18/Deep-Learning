# üáÆüá≥ English ‚Üí Hindi Translation using Transformer

This project demonstrates training and inference of a Transformer-based machine translation model on an **English-Hindi parallel corpus** using Hugging Face‚Äôs Transformers library.

---

## Dataset
- Format: CSV file
- Columns:
    - `english_sentence`: English sentence
    - `hindi_sentence`: Hindi sentence
- Example row:
    | English | Hindi |
    |---------|-------|
    | Hello, how are you? | ‡§®‡§Æ‡§∏‡•ç‡§§‡•á, ‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•à‡§Ç? |

---

## Approach

1. **Data Preparation**  
   Loaded the CSV dataset and tokenized sentences using Hugging Face‚Äôs `AutoTokenizer` from the pretrained model `Helsinki-NLP/opus-mt-en-hi`.

2. **Pretrained Model Used**  
   Model: `Helsinki-NLP/opus-mt-en-hi`  
   This is a MarianMT (Marian Machine Translation) model pretrained specifically for English ‚Üí Hindi translation.

3. **Model Architecture**  
   Utilizes Transformer encoder-decoder architecture with:
   - Token embeddings
   - Positional encoding
   - Multi-head self-attention layers
   - Final linear layer projecting to vocabulary logits

4. **Training**
    - Fine-tuned the pretrained model on the dataset.
    - Batch size: 8
    - Loss: Cross-Entropy
    - Optimizer: AdamW
    - Teacher forcing used during training

5. **Inference**
    - Input sentence tokenized and passed to the model.
    - Translation generated using `model.generate()`.
    - Output tokens decoded to Hindi text.

---

##  Example Result

| English Input               | Hindi Output                |
|-----------------------------|-----------------------------|
| How are you today?          | ‡§Ü‡§™ ‡§Ü‡§ú ‡§ï‡•à‡§∏‡•á ‡§π‡•à‡§Ç?              |
| I love machine learning.    | ‡§Æ‡•Å‡§ù‡•á ‡§Æ‡§∂‡•Ä‡§® ‡§≤‡§∞‡•ç‡§®‡§ø‡§Ç‡§ó ‡§™‡§∏‡§Ç‡§¶ ‡§π‡•à‡•§  |
| What is your name?          | ‡§Ü‡§™‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?            |

---

##  Key Insights

- Pretrained MarianMT models provide state-of-the-art translation quality out of the box.
- Fine-tuning helps the model adapt to your specific dataset with small compute requirements.
- Transformer architecture handles long-range dependencies better than RNNs.
- Tokenizer handles padding, truncation, and subword tokenization efficiently.

---

